{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN3CU7Mb/pBAkjkaU/CTqTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Az/TransformerLLM-Finetuning/blob/main/NanoGPT_TransformerLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A 'Nano' Generative Pre-trained Transformer LLM\n",
        "This is an experiment where I create and train a mini GPT model from scratch. It will be using a ebook of my choice as its' training data."
      ],
      "metadata": {
        "id": "wg_AjMGGo_sC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "p3tbFB6Ro8cQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Paf9FeBoepaw"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bF183rJjtq64"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "rxTk4YGb153S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting, Transforming, and Loading the Ebook Data\n",
        "The book I chose is \"Voices from within the Veil\" by W.E.B DuBois. This book was taken from the Project Gutenberg website and is available to use freely. I uploaded it to Google Drive for my own use as to not scrape project gutenberg's website."
      ],
      "metadata": {
        "id": "ts6B5Fe6lr03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file_from_google_drive(glink):\n",
        "    id = glink.split('/')[-2]\n",
        "    filepath = None\n",
        "    try:\n",
        "        filepath = gdown.download(id=id, quiet=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "    ebook = open(filepath, 'r').readlines()\n",
        "    return ''.join(ebook)"
      ],
      "metadata": {
        "id": "EAWsGeKhAQZj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voices_within_veil_glink = 'https://drive.google.com/file/d/1Ah7n0r-bjW8nbTdc82C8_9_4l3c9lOql/view?usp=sharing'\n",
        "voices_within_veil = download_file_from_google_drive(voices_within_veil_glink)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1E1YHZOkWxo",
        "outputId": "73e67c5c-232a-4838-a21a-f59697c6c702"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ah7n0r-bjW8nbTdc82C8_9_4l3c9lOql\n",
            "To: /content/Voices_from_within_the_Veil_WEBDuBois.txt\n",
            "100%|██████████| 419k/419k [00:00<00:00, 89.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(voices_within_veil[50:305])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VdZSRaUoZoW",
        "outputId": "e5d1fa8e-9392-405f-8c63-b533c664943c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from Within the Veil\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(voices_within_veil))) #chars and vocab size are parameters used within the model and tokenizer\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpt3DYyXuYKn",
        "outputId": "fa2167c7-bc91-4c5c-f0ec-9c35b4399c11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzàäçéëñô—‘’“”•™﻿\n",
            "98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tokenizer using a simple encoding schema\n",
        "class Tokenizer:\n",
        "    def __init__(self, chars):\n",
        "        self.chars = chars\n",
        "        self.char_to_token = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.token_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def encode(self, input_text):\n",
        "        \"\"\"Encodes a string into a list of integers.\n",
        "\n",
        "        Args:\n",
        "            input_text: The string to encode.\n",
        "\n",
        "        Returns:\n",
        "            A list of integers representing the encoded string.\n",
        "        \"\"\"\n",
        "        return [self.char_to_token[c] for c in input_text]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decodes a list of integers into a string.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of integers to decode.\n",
        "\n",
        "        Returns:\n",
        "            The decoded string.\n",
        "        \"\"\"\n",
        "        return ''.join([self.token_to_char[i] for i in tokens])"
      ],
      "metadata": {
        "id": "0t2xDDsfmiDT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(chars)"
      ],
      "metadata": {
        "id": "fky20aKgD5sL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the data\n",
        "encoded_voices_within_veil = tokenizer.encode(voices_within_veil)"
      ],
      "metadata": {
        "id": "BATFFjbkqe6t"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the encoded text into a tensor\n",
        "encoded_voices_within_veil = torch.tensor(encoded_voices_within_veil)"
      ],
      "metadata": {
        "id": "gPvTNGB5r9RB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the ebook into train and test splits\n",
        "percent_train = 0.9\n",
        "train_data = encoded_voices_within_veil[:int(percent_train*len(encoded_voices_within_veil))]\n",
        "test_data = encoded_voices_within_veil[int(percent_train*len(encoded_voices_within_veil)):]"
      ],
      "metadata": {
        "id": "_3pbb2jnnyej"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"Size of train, test splits: {len(train_data), len(test_data)}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XepFktwvoG4C",
        "outputId": "86c39420-cf42-49c7-8b1f-c3362bae082c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Size of train, test splits: (369613, 41069)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent the train and test data to gpu device\n",
        "train_data = train_data.to(device)\n",
        "test_data = test_data.to(device)"
      ],
      "metadata": {
        "id": "kvwbK6WX1uHZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to batch the data so that the first N words are 'context' and the final N+1 word is the 'target'.\n",
        "batch_size = 4 # independent sequences processed in parallel\n",
        "block_size = 8 # maximum context length for predictions\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch; inputs x and targets y\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size - 3): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJo_y44JpkT3",
        "outputId": "4a0d7600-ddbe-43ac-f074-cd3c90fe1d8f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [61,  1, 64, 57, 78, 61,  1, 76],\n",
            "        [82, 57, 76, 65, 71, 70,  1, 71],\n",
            "        [76, 61,  0,  3, 72, 74, 61, 75]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 1, 64, 57, 78, 61,  1, 76, 71],\n",
            "        [57, 76, 65, 71, 70,  1, 71, 62],\n",
            "        [61,  0,  3, 72, 74, 61, 75, 76]], device='cuda:0')\n",
            "----\n",
            "when input is [1] the target: 1\n",
            "when input is [1, 1] the target: 1\n",
            "when input is [1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1, 1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1, 1, 1, 1, 1] the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Mini Transformer Model"
      ],
      "metadata": {
        "id": "4Ys5oZj-tZMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "tjMEUzLat2IM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hLppGmygtmgm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "P9sKR-XguGyi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "d44SMR07uJf4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HumFA1RRuMvT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        idx = torch.clamp(idx, 0, self.token_embedding_table.num_embeddings - 1)\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            # Ensure targets are within the valid range for logits\n",
        "            targets = torch.clamp(targets, 0, C - 1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "CMyeXqFStUeT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Mini Transformer LLM with Self-Attention for 10,000 Steps"
      ],
      "metadata": {
        "id": "ehJaWYEgumJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "WsL44WiEoJIX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nanoGPT_model = BigramLanguageModel()\n",
        "nanoGPT_model = nanoGPT_model.to(device)"
      ],
      "metadata": {
        "id": "jcyb9MyZ21y5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the number of parameters in the model per million\n",
        "print(sum(p.numel() for p in nanoGPT_model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# using the pyTorch AdamW optimizer\n",
        "optimizer = torch.optim.AdamW(nanoGPT_model.parameters(), lr=learning_rate)\n",
        "eval_interval = 1000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # evaluate the model and print the loss every 1000 steps\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(nanoGPT_model)\n",
        "        if iter % eval_interval == 0:\n",
        "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = nanoGPT_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG7d0En5uqdn",
        "outputId": "e782061b-ffdc-4c87-c68e-ac8be6d03327"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213986 M parameters\n",
            "step 0: train loss 4.7520, val loss 4.7472\n",
            "step 1000: train loss 2.0895, val loss 2.3192\n",
            "step 2000: train loss 1.8783, val loss 2.1746\n",
            "step 3000: train loss 1.7980, val loss 2.0762\n",
            "step 4000: train loss 1.7139, val loss 2.0313\n",
            "step 5000: train loss 1.6635, val loss 1.9782\n",
            "step 6000: train loss 1.6308, val loss 1.9866\n",
            "step 7000: train loss 1.6092, val loss 1.9630\n",
            "step 8000: train loss 1.5946, val loss 1.9439\n",
            "step 9000: train loss 1.5648, val loss 1.9124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text Completion from the Nano-GPT Model"
      ],
      "metadata": {
        "id": "JIBfIdtw5J-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'Voices from within the'\n",
        "expected_output = 'Voices from within the Veil'"
      ],
      "metadata": {
        "id": "kcdLLbys4b8Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor(tokenizer.encode(input), dtype=torch.long, device=device).unsqueeze(0)"
      ],
      "metadata": {
        "id": "p13gobB14ral"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(nanoGPT_model.generate(context, max_new_tokens=20)[0].tolist())"
      ],
      "metadata": {
        "id": "CrI75Pew4EfB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input: {input}\")\n",
        "print(f\"Expected Output: {expected_output}\")\n",
        "print(f\"Model Output: {output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sap2dR8T4Nhi",
        "outputId": "59e2c9b8-0ef6-470a-d709-ccdfe20711be"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Voices from within the\n",
            "Expected Output: Voices from within the Veil\n",
            "Model Output: Voices from within the givered ramily line\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xVsZXOqGjBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}